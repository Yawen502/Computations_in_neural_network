{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of models\n",
    "- Full GRU\n",
    "- Simple GRU\n",
    "- CB-GRU\n",
    "- CB-RNN-tied\n",
    "- Dale-CB\n",
    "- CB-RNN-tied-STP /\n",
    "- Dale-CB-STP /\n",
    "- Vanilla RNN\n",
    "\n",
    "### Variants\n",
    "With 24 neurons / with 48 neurons (let's do 48 first)\n",
    "\n",
    "### Check Features\n",
    "Input/ Ouput neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "sequence_length =28*28//input_size\n",
    "hidden_size = 48\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "stride_number = 4\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "per = np.random.permutation(28*28)\n",
    "\n",
    "def permuted(img, per):\n",
    "    'transform a 28*28 image to a 28*28 permuted image'\n",
    "    channels, rows, cols = img.shape\n",
    "    order = np.random.permutation(rows*cols)\n",
    "    permuted = np.zeros((channels, rows, cols), dtype=img.dtype)\n",
    "    for c in range(channels):\n",
    "        permuted[c, :, :] = img[c, :, :].flatten()[per-1].reshape(rows, cols)\n",
    "\n",
    "    return permuted.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, (images, labels) in enumerate(loaders['train']):\\n    images = images.reshape(-1, sequence_length, input_size).to(device)\\n    images = stride(images, stride_number).to(device)\\n    print(images.shape)\\n    print(labels.shape)\\n    print(len(loaders['train']))\\n    break\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Data Preprocessing'\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# get index of currently selected device\n",
    "print(torch.cuda.current_device()) # returns 0 in my case\n",
    "\n",
    "# get number of GPUs available\n",
    "print(torch.cuda.device_count()) # returns 1 in my case\n",
    "\n",
    "# get the name of the device\n",
    "print(torch.cuda.get_device_name(0)) # good old Tesla K80\n",
    "\n",
    "def snake_scan(img):\n",
    "    'Converts a 32x32 image to a 32x96 array with snake-like row ordering'\n",
    "    if len(img.shape) != 3:\n",
    "        raise ValueError('Input image must be a 3D array')\n",
    "    \n",
    "    channels, rows, cols = img.shape\n",
    "    snake = np.zeros((rows, cols * channels), dtype=img.dtype)\n",
    "    for r in range(rows):\n",
    "        row_data = img[:, r, :].flatten()  # Flattening each row into a 1D array of 96 elements\n",
    "        if r % 2 == 1:\n",
    "            row_data = row_data[::-1]  # Reversing the order for alternate rows\n",
    "        snake[r] = row_data\n",
    "    return snake\n",
    "\n",
    "def stride(input_data, stride):\n",
    "    'turn [batch_size, sequence_length, input_size] into [batch_size, sequence_length*input_size/stride, input_size]'\n",
    "    batch_size, sequence_length, input_size = input_data.shape\n",
    "    # flatten the input data to put sequence and input size together\n",
    "    input_data = input_data.reshape(batch_size, -1)\n",
    "    # append zeros to make sure the last pixel can be fed as the first pixel of the next sequence\n",
    "    n = input_size - (sequence_length*input_size)%stride\n",
    "\n",
    "    input_data = input_data.cpu()\n",
    "    input_data = input_data.numpy()\n",
    "    input_data = np.append(input_data, np.zeros((batch_size, n)), axis=1)\n",
    "    input_data = torch.tensor(input_data)\n",
    "    #print(input_data.shape)\n",
    "    output_data = torch.zeros(batch_size, sequence_length*input_size//stride, input_size)\n",
    "    for i in range(sequence_length*input_size//stride):\n",
    "        # if stride = input size, then the output data is the same as input data\n",
    "        #print(i)\n",
    "\n",
    "        output_data[:,i,:] = input_data[:,i*stride:i*stride+input_size]\n",
    "        #print(output_data[batch,i,:])\n",
    "\n",
    "    return output_data\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: torch.tensor(permuted(x.numpy(), per)))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    download = True,\n",
    "    transform = transform     \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    download = True,\n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "'Hyperparameters'\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=0),\n",
    "}\n",
    "loaders\n",
    "'''\n",
    "for i, (images, labels) in enumerate(loaders['train']):\n",
    "    images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "    images = stride(images, stride_number).to(device)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    print(len(loaders['train']))\n",
    "    break\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class simple_GRU_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(simple_GRU_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # Wz is defined in the forward function\n",
    "        self.W_z = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))         \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = self.Sigmoid(torch.matmul(self.W_z, self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class simple_GRU_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(simple_GRU_batch, self).__init__()\n",
    "        self.rnncell = simple_GRU_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class simple_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(simple_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = simple_GRU_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = simple_GRU(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_GRU(\n",
      "  (lstm): simple_GRU_batch(\n",
      "    (rnncell): simple_GRU_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 67.00\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 78.50\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 81.00\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 81.50\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 83.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 83.30\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 84.40\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 84.30\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 85.90\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 84.30\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 85.30\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:87.02%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Timescale RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinearity: Sigmoid\n",
    "\n",
    "$z_t = \\sigma (W_z r_t + P_zx_t + b_z)$ with $W_z = P_z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class multiscale_RNN_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(multiscale_RNN_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # Wz is defined in the forward function\n",
    "        self.W_z = torch.nn.Parameter(torch.zeros(self.hidden_size, self.hidden_size), requires_grad=False)\n",
    "        self.P_z = torch.nn.Parameter(torch.zeros(self.hidden_size, input_size), requires_grad=False)\n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))         \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "        self.z_low = torch.tensor(0.1)\n",
    "        self.z_high = torch.tensor(0.9)\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)*self.Sigmoid(torch.matmul(self.W_z, self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class multiscale_RNN_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(multiscale_RNN_batch, self).__init__()\n",
    "        self.rnncell = multiscale_RNN_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class multiscale_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(multiscale_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = multiscale_RNN_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = multiscale_RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiscale_RNN(\n",
      "  (lstm): multiscale_RNN_batch(\n",
      "    (rnncell): multiscale_RNN_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 22.20\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 34.70\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 44.30\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 48.70\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 51.40\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 51.70\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 55.10\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 59.60\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 64.10\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 65.30\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 67.90\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 69.30\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 69.40\n",
      "Epoch [7/10], Step [1500/1500], Training Accuracy: 71.20\n",
      "Epoch [8/10], Step [750/1500], Training Accuracy: 74.80\n",
      "Epoch [8/10], Step [1500/1500], Training Accuracy: 73.30\n",
      "Epoch [9/10], Step [750/1500], Training Accuracy: 75.00\n",
      "Epoch [9/10], Step [1500/1500], Training Accuracy: 75.30\n",
      "Epoch [10/10], Step [750/1500], Training Accuracy: 74.60\n",
      "Epoch [10/10], Step [1500/1500], Training Accuracy: 77.10\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:77.38%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_1_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_1_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(1.0)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_1_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_1_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_1_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_1(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_1_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_1(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_1(\n",
      "  (lstm): vanilla_RNN_1_batch(\n",
      "    (rnncell): vanilla_RNN_1_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 57.90\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 69.50\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 74.40\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 77.20\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 78.60\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 80.10\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 80.30\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 82.00\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 81.00\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 83.80\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 83.10\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 81.00\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:82.73%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_2_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_2_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(0.5)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_2_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_2_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_2_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_2_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_2(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_2(\n",
      "  (lstm): vanilla_RNN_2_batch(\n",
      "    (rnncell): vanilla_RNN_2_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 41.50\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 61.90\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 70.20\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 74.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 76.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 79.70\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 79.30\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 78.60\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:80.62%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_3_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_3_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(0.1)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_3_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_3_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_3_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_3(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_3_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_3(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_3(\n",
      "  (lstm): vanilla_RNN_3_batch(\n",
      "    (rnncell): vanilla_RNN_3_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 10.30\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 11.40\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 23.60\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 25.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 36.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 35.50\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 47.40\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 51.10\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 55.60\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 54.90\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 58.30\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 61.10\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 62.70\n",
      "Epoch [7/10], Step [1500/1500], Training Accuracy: 63.60\n",
      "Epoch [8/10], Step [750/1500], Training Accuracy: 66.30\n",
      "Epoch [8/10], Step [1500/1500], Training Accuracy: 67.20\n",
      "Epoch [9/10], Step [750/1500], Training Accuracy: 69.40\n",
      "Epoch [9/10], Step [1500/1500], Training Accuracy: 68.90\n",
      "Epoch [10/10], Step [750/1500], Training Accuracy: 70.80\n",
      "Epoch [10/10], Step [1500/1500], Training Accuracy: 70.70\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:73.45%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "\n",
    "class CB_GRU_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_GRU_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K is always positive            \n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))     \n",
    "        self.K = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))\n",
    "        self.z_high = torch.tensor(0.005)\n",
    "        self.z_low = torch.tensor(1.0)\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.Sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "        # No sign constraint on K and W\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "        \n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = (self.z_high-self.z_low)* self.Sigmoid(torch.matmul(self.K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z) + self.z_low\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)                \n",
    "\n",
    "class CB_GRU_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_GRU_batch, self).__init__()\n",
    "        self.rnncell = CB_GRU_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_GRU_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = CB_GRU(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_GRU(\n",
      "  (lstm): CB_GRU_batch(\n",
      "    (rnncell): CB_GRU_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 62.50\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 71.00\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 74.30\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 78.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 80.80\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 81.10\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 79.40\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 83.10\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 81.50\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 84.30\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 84.80\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 78.10\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 83.40\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:84.53%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n",
    "\n",
    "# Retrieve weights\n",
    "P = model.lstm.rnncell.P.detach().cpu().numpy()\n",
    "W = model.lstm.rnncell.W.detach().cpu().numpy()\n",
    "read_out = model.fc.weight.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-RNN-tied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CB_RNN_tiedcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_RNN_tiedcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.W = torch.nn.Parameter(torch.empty(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and P_z become tied          \n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # initialise e as a random float between 0 and 1\n",
    "        self.e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_p = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "        # Voltage rate\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        for w in self.W, self.P:\n",
    "            glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        e = self.softplus(self.e)\n",
    "        e_p = self.softplus(self.e_p)\n",
    "        K = e * self.softplus(self.W)\n",
    "        P_z = e_p * self.softplus(self.P)\n",
    "\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low) * self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(P_z, x) + self.b_z)\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)                \n",
    "\n",
    "class CB_RNN_tied_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_RNN_tied_batch, self).__init__()\n",
    "        self.rnncell = CB_RNN_tiedcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_RNN_tied(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_RNN_tied, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_RNN_tied_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = CB_RNN_tied(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_RNN_tied(\n",
      "  (lstm): CB_RNN_tied_batch(\n",
      "    (rnncell): CB_RNN_tiedcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 55.10\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 67.90\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 73.00\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 75.90\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 79.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 79.30\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 80.40\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 80.30\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 81.80\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 80.80\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 83.40\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 83.70\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 83.10\n",
      "Epoch [7/10], Step [1500/1500], Training Accuracy: 84.00\n",
      "Epoch [8/10], Step [750/1500], Training Accuracy: 83.50\n",
      "Epoch [8/10], Step [1500/1500], Training Accuracy: 84.90\n",
      "Epoch [9/10], Step [750/1500], Training Accuracy: 81.60\n",
      "Epoch [9/10], Step [1500/1500], Training Accuracy: 83.90\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:85.55%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dale-CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dale_CBcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Dale_CBcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and W are unbounded free parameters   \n",
    "        # C represents  current based portion of connectivity       \n",
    "        self.K = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "        self.C = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # Potentials are initialised with right signs\n",
    "        self.e_e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_i = torch.nn.Parameter(-torch.rand(1))\n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "        # initialise matrices\n",
    "        # P and P_z are unconstrained\n",
    "        for w in self.P_z, self.P:\n",
    "            glorot_init(w)\n",
    "        for w in self.K, self.C:\n",
    "            positive_glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "    def init_dale(self, rows, cols):\n",
    "        # Dale's law with equal excitatory and inhibitory neurons\n",
    "        exci = torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        inhi = -torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        weights = torch.cat((exci, inhi), dim=1)\n",
    "        weights = self.adjust_spectral(weights)\n",
    "        return weights\n",
    "\n",
    "    def adjust_spectral(self, weights, desired_radius=1.5):\n",
    "        #values, _ = torch.linalg.eig(weights @ weights.T)\n",
    "        values = torch.linalg.svdvals(weights)\n",
    "        radius = values.abs().max()\n",
    "        return weights * (desired_radius / radius)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        K = self.softplus(self.K)\n",
    "        C = self.softplus(self.C)\n",
    "        # W is constructed using e*(K+C)\n",
    "        W_E = self.e_e * (K[:, :self.hidden_size//2] + C[:, :self.hidden_size//2])\n",
    "        W_I = self.e_i * (K[:, self.hidden_size//2:] + C[:, self.hidden_size//2:])\n",
    "        # print to see which device the tensor is on\n",
    "        # If sign of W do not obey Dale's law, then these terms to be 0\n",
    "        W_E = self.relu(W_E)\n",
    "        W_I = -self.relu(-W_I)\n",
    "        W = torch.cat((W_E, W_I), 1)\n",
    "        self.W = W\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//4:self.hidden_size//2,:] = 0\n",
    "        input_mask[3*self.hidden_size//4:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)* self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "        excitatory = self.v_t[:, :self.hidden_size//2]\n",
    "        self.excitatory = torch.cat((excitatory, torch.zeros_like(excitatory)), 1)   \n",
    "\n",
    "class Dale_CB_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(Dale_CB_batch, self).__init__()\n",
    "        self.rnncell = Dale_CBcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.excitatory            \n",
    "            \n",
    "class Dale_CB(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(Dale_CB, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = Dale_CB_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,:self.hidden_size//4] = 0\n",
    "        output_mask[:,3*self.hidden_size//4:] = 0        \n",
    "        out = out * output_mask\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = Dale_CB(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dale_CB(\n",
      "  (lstm): Dale_CB_batch(\n",
      "    (rnncell): Dale_CBcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 10.30\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 9.60\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:80.93%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-RNN-tied-STP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "\n",
    "class CB_RNN_tiedcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_RNN_tiedcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.W = torch.nn.Parameter(torch.empty(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and P_z become tied          \n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # initialise e as a random float between 0 and 1\n",
    "        self.e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_p = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "        # Voltage rate\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        for w in self.W, self.P:\n",
    "            glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        ### STP Model ###\n",
    "        self.delta_t = 1\n",
    "        self.z_min = 0.001\n",
    "        self.z_max = 0.1\n",
    "\n",
    "        # Short term Depression parameters  \n",
    "        self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "        # Short term Facilitation parameters\n",
    "        self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        \n",
    "        # State initialisations\n",
    "        self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32).to(device)\n",
    "        self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32).to(device)\n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.Ucapclone = self.Ucap.clone().detach() \n",
    "\n",
    "        #self.X_history = []\n",
    "        #self.U_history = []\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        e = self.softplus(self.e)\n",
    "        e_p = self.softplus(self.e_p)\n",
    "        K = e * self.softplus(self.W)\n",
    "        P_z = e_p * self.softplus(self.P)\n",
    "\n",
    "        ### STP model ###\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        \n",
    "        # Short term Depression \n",
    "        self.z_x = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_x)\n",
    "        self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.r_t\n",
    "\n",
    "        # Short term Facilitation \n",
    "        self.z_u = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_u)    \n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.r_t\n",
    "        self.Ucapclone = self.Ucap.clone().detach()\n",
    "        self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)* self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(P_z, x) + self.b_z)\n",
    "        # mask p with second half of the neuron not receiving input\n",
    "\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.U*self.X*self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "\n",
    "        #self.X_history.append(self.X.clone().detach())\n",
    "        #self.U_history.append(self.U.clone().detach())\n",
    "        #self.v_t_history.append(self.v_t.clone().detach())\n",
    "        #self.z_t_history.append(self.z_t.clone().detach())       \n",
    "\n",
    "class CB_RNN_tied_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_RNN_tied_batch, self).__init__()\n",
    "        self.rnncell = CB_RNN_tiedcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_RNN_tied(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_RNN_tied, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_RNN_tied_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.X = torch.ones(self.hidden_size, x.size(0), dtype=torch.float32).to(device)\n",
    "        self.lstm.rnncell.U = (self.lstm.rnncell.Ucapclone.repeat(1, x.size(0))).to(device)\n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        # mask only the second half giving output\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, hidden_size)\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = CB_RNN_tied(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_RNN_tied(\n",
      "  (lstm): CB_RNN_tied_batch(\n",
      "    (rnncell): CB_RNN_tiedcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 29.20\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 44.90\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 51.60\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 62.90\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 68.40\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 74.80\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 73.20\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 74.90\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 77.70\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 77.60\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 76.70\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:77.93%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "X_history = []\n",
    "U_history = []\n",
    "v_t_history = []\n",
    "z_t_history = []\n",
    "labelslist = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.X_history = []\n",
    "        model.lstm.rnncell.U_history = []\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "        X_history.append(model.lstm.rnncell.X_history)\n",
    "        U_history.append(model.lstm.rnncell.U_history)\n",
    "        v_t_history.append(model.lstm.rnncell.v_t_history)\n",
    "        z_t_history.append(model.lstm.rnncell.z_t_history)\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dale-CB-STP\n",
    "Accuracy of the model:55.56% (doubled neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9006, grad_fn=<MulBackward0>)\n",
      "tensor(0.9000, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Dale_CB_STPcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Dale_CB_STPcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and W are unbounded free parameters   \n",
    "        # C represents  current based portion of connectivity       \n",
    "        while True:\n",
    "            self.K = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "            self.C = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "            nse_K = self.NSE(self.K)\n",
    "            nse_C = self.NSE(self.C)\n",
    "            if nse_K > 0.87 and nse_C > 0.87:\n",
    "                break\n",
    "        self.P_z = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # Potentials are initialised with right signs\n",
    "        self.e_e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_i = torch.nn.Parameter(-torch.rand(1))\n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        # P and P_z are unconstrained\n",
    "        for w in self.P_z, self.P:\n",
    "            glorot_init(w)\n",
    "        for w in self.K, self.C:\n",
    "            positive_glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        ### STP Model ###\n",
    "        self.delta_t = 1\n",
    "        self.z_min = 0.001\n",
    "        self.z_max = 0.1\n",
    "\n",
    "        # Short term Depression parameters  \n",
    "        self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "        # Short term Facilitation parameters\n",
    "        self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        \n",
    "        # State initialisations\n",
    "        self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32).to(device)\n",
    "        self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32).to(device)\n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.Ucapclone = self.Ucap.clone().detach() \n",
    "\n",
    "        #self.X_history = []\n",
    "        #self.U_history = []\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "    def init_dale(self, rows, cols):\n",
    "        # Dale's law with equal excitatory and inhibitory neurons\n",
    "        exci = torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        inhi = -torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        weights = torch.cat((exci, inhi), dim=1)\n",
    "        weights = self.adjust_spectral(weights)\n",
    "        return weights\n",
    "\n",
    "    def adjust_spectral(self, weights, desired_radius=1.5):\n",
    "        values= torch.linalg.svdvals(weights)\n",
    "        radius = values.abs().max()\n",
    "        return weights * (desired_radius / radius)\n",
    "    \n",
    "    def NSE(self, weights):\n",
    "        values = torch.linalg.svdvals(weights)\n",
    "        normalised_v = values/sum(values)\n",
    "        H = -1/torch.log(torch.tensor(self.hidden_size)) * torch.sum(normalised_v * torch.log(normalised_v))\n",
    "        print(H)\n",
    "        return H\n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        K = self.softplus(self.K)\n",
    "        C = self.softplus(self.C)\n",
    "        # W is constructed using e*(K+C)\n",
    "        # first half of neurons are excitatory and second half are inhibitory\n",
    "        W_E = self.e_e * (K[:, :self.hidden_size//2] + C[:, :self.hidden_size//2])\n",
    "        W_I = self.e_i * (K[:, self.hidden_size//2:] + C[:, self.hidden_size//2:])\n",
    "        # print to see which device the tensor is on\n",
    "        # If sign of W do not obey Dale's law, then these terms to be 0\n",
    "        W_E = self.relu(W_E)\n",
    "        W_I = -self.relu(-W_I)\n",
    "        W = torch.cat((W_E, W_I), 1)\n",
    "        self.W = W\n",
    "\n",
    "        ### STP model ###\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        \n",
    "        # Short term Depression \n",
    "        self.z_x = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_x)\n",
    "        self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.r_t\n",
    "\n",
    "        # Short term Facilitation \n",
    "        self.z_u = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_u)    \n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.r_t\n",
    "        self.Ucapclone = self.Ucap.clone().detach()\n",
    "        self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "\n",
    "\n",
    "        ### Update Equations ###\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low) * self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "        \n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//4:self.hidden_size//2,:] = 0\n",
    "        input_mask[3*self.hidden_size//4:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(W, self.U*self.X*self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "        excitatory = self.v_t[:, :self.hidden_size//2]\n",
    "        self.excitatory = torch.cat((excitatory, torch.zeros_like(excitatory)), 1)    \n",
    "\n",
    "        #self.X_history.append(self.X.clone().detach())\n",
    "        #self.U_history.append(self.U.clone().detach())\n",
    "        #self.v_t_history.append(self.v_t.clone().detach())\n",
    "        #self.z_t_history.append(self.z_t.clone().detach())   \n",
    "\n",
    "\n",
    "class Dale_CB_STP_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(Dale_CB_STP_batch, self).__init__()\n",
    "        self.rnncell = Dale_CB_STPcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.excitatory            \n",
    "            \n",
    "class Dale_CB_STP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(Dale_CB_STP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = Dale_CB_STP_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.X = torch.ones(self.hidden_size, x.size(0), dtype=torch.float32).to(device)\n",
    "        self.lstm.rnncell.U = (self.lstm.rnncell.Ucapclone.repeat(1, x.size(0))).to(device)\n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, hidden_size)\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,:self.hidden_size//4] = 0\n",
    "        output_mask[:,3*self.hidden_size//4:] = 0        \n",
    "        out = out * output_mask\n",
    "\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = Dale_CB_STP(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dale_CB_STP(\n",
      "  (lstm): Dale_CB_STP_batch(\n",
      "    (rnncell): Dale_CB_STPcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 21.10\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 36.20\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 39.90\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 53.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 58.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 60.40\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 65.50\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 65.90\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 67.00\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 68.00\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 69.60\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 67.90\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 69.10\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:71.63%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "X_history = []\n",
    "U_history = []\n",
    "v_t_history = []\n",
    "z_t_history = []\n",
    "labelslist = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.X_history = []\n",
    "        model.lstm.rnncell.U_history = []\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "        X_history.append(model.lstm.rnncell.X_history)\n",
    "        U_history.append(model.lstm.rnncell.U_history)\n",
    "        v_t_history.append(model.lstm.rnncell.v_t_history)\n",
    "        z_t_history.append(model.lstm.rnncell.z_t_history)\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance of permuted MNIST')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# rotate model name\n",
    "\n",
    "model_name = ['simple GRU', 'CB-GRU', 'CB-RNN-tied', 'Dale-CB', 'CB-RNN-tied STP', 'Dale-CB STP', 'multiscale-Vanilla RNN', 'Vanilla RNN $z=1.0$', 'Vanilla RNN $z=0.5$', 'Vanilla RNN $z=0.1$']\n",
    "perf1 = [87.02, 84.53, 85.55, 0, 0, 0, 77.38, 82.73, 80.62, 73.45]\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(model_name, perf1, bar_width, label='48 Neurons')\n",
    "index = np.arange(len(model_name))\n",
    "# text bar to show input size, stride number, and hidden size\n",
    "plt.text(4.5, 80, f'Input size: {input_size}\\nStride number: {stride_number}'.format(input_size=input_size, stride_number=stride_number, hidden_size=hidden_size), ha='center', va='bottom')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance of permuted MNIST')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
