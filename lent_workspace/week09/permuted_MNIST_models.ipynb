{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of models\n",
    "- Full GRU\n",
    "- Simple GRU\n",
    "- CB-GRU\n",
    "- CB-RNN-tied\n",
    "- Dale-CB\n",
    "- CB-RNN-tied-STP /\n",
    "- Dale-CB-STP /\n",
    "- Vanilla RNN\n",
    "\n",
    "### Variants\n",
    "With 24 neurons / with 48 neurons (let's do 48 first)\n",
    "\n",
    "### Check Features\n",
    "Input/ Ouput neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "sequence_length =28*28//input_size\n",
    "hidden_size = 48\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 40\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "stride_number = 2\n",
    "timegap = 48\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "per = np.random.permutation(28*28)\n",
    "\n",
    "def permuted(img, per):\n",
    "    'transform a 28*28 image to a 28*28 permuted image'\n",
    "    channels, rows, cols = img.shape\n",
    "    order = np.random.permutation(rows*cols)\n",
    "    permuted = np.zeros((channels, rows, cols), dtype=img.dtype)\n",
    "    for c in range(channels):\n",
    "        permuted[c, :, :] = img[c, :, :].flatten()[per-1].reshape(rows, cols)\n",
    "\n",
    "    return permuted.squeeze()\n",
    "\n",
    "def time_gap(img, input_size, n):\n",
    "    ' feed image in a manner 1, 3, 5, 7, 2,4,6,8... for input size 4 and n=2' \n",
    "    batch, sequence_length, input_size = img.shape\n",
    "    # keep batch size as it is, flatten other two dimensions\n",
    "    img = img.reshape(batch, sequence_length*input_size)\n",
    "    # create a new tensor to hold the new sequence\n",
    "    new_img = torch.zeros((batch, sequence_length,input_size))\n",
    "    # fill the new sequence tensor with the image\n",
    "    for j in range(sequence_length):\n",
    "        new = img[:, j::n]\n",
    "        new_img[:, j,:] = new[:,:input_size]\n",
    "    # reshape the tensor to original shape\n",
    "    new_img = new_img.reshape(batch, sequence_length, input_size)\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, (images, labels) in enumerate(loaders['train']):\\n    images = images.reshape(-1, sequence_length, input_size).to(device)\\n    images = stride(images, stride_number).to(device)\\n    print(images.shape)\\n    print(labels.shape)\\n    print(len(loaders['train']))\\n    break\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Data Preprocessing'\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# get index of currently selected device\n",
    "print(torch.cuda.current_device()) # returns 0 in my case\n",
    "\n",
    "# get number of GPUs available\n",
    "print(torch.cuda.device_count()) # returns 1 in my case\n",
    "\n",
    "# get the name of the device\n",
    "print(torch.cuda.get_device_name(0)) # good old Tesla K80\n",
    "\n",
    "def snake_scan(img):\n",
    "    'Converts a 32x32 image to a 32x96 array with snake-like row ordering'\n",
    "    if len(img.shape) != 3:\n",
    "        raise ValueError('Input image must be a 3D array')\n",
    "    \n",
    "    channels, rows, cols = img.shape\n",
    "    snake = np.zeros((rows, cols * channels), dtype=img.dtype)\n",
    "    for r in range(rows):\n",
    "        row_data = img[:, r, :].flatten()  # Flattening each row into a 1D array of 96 elements\n",
    "        if r % 2 == 1:\n",
    "            row_data = row_data[::-1]  # Reversing the order for alternate rows\n",
    "        snake[r] = row_data\n",
    "    return snake\n",
    "\n",
    "def stride(input_data, stride):\n",
    "    'turn [batch_size, sequence_length, input_size] into [batch_size, sequence_length*input_size/stride, input_size]'\n",
    "    batch_size, sequence_length, input_size = input_data.shape\n",
    "    # flatten the input data to put sequence and input size together\n",
    "    input_data = input_data.reshape(batch_size, -1)\n",
    "    # append zeros to make sure the last pixel can be fed as the first pixel of the next sequence\n",
    "    n = input_size - (sequence_length*input_size)%stride\n",
    "\n",
    "    input_data = input_data.cpu()\n",
    "    input_data = input_data.numpy()\n",
    "    input_data = np.append(input_data, np.zeros((batch_size, n)), axis=1)\n",
    "    input_data = torch.tensor(input_data)\n",
    "    #print(input_data.shape)\n",
    "    output_data = torch.zeros(batch_size, sequence_length*input_size//stride, input_size)\n",
    "    for i in range(sequence_length*input_size//stride):\n",
    "        # if stride = input size, then the output data is the same as input data\n",
    "        #print(i)\n",
    "\n",
    "        output_data[:,i,:] = input_data[:,i*stride:i*stride+input_size]\n",
    "        #print(output_data[batch,i,:])\n",
    "\n",
    "    return output_data\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: torch.tensor(snake_scan(x.numpy())))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    download = True,\n",
    "    transform = transform     \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    download = True,\n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "'Hyperparameters'\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=0),\n",
    "}\n",
    "loaders\n",
    "'''\n",
    "for i, (images, labels) in enumerate(loaders['train']):\n",
    "    images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "    images = stride(images, stride_number).to(device)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    print(len(loaders['train']))\n",
    "    break\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class simple_GRU_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(simple_GRU_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # Wz is defined in the forward function\n",
    "        self.W_z = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))         \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = self.Sigmoid(torch.matmul(self.W_z, self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class simple_GRU_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(simple_GRU_batch, self).__init__()\n",
    "        self.rnncell = simple_GRU_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class simple_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(simple_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = simple_GRU_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = simple_GRU(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_GRU(\n",
      "  (lstm): simple_GRU_batch(\n",
      "    (rnncell): simple_GRU_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=150, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/20], Step [750/1500], Training Accuracy: 65.20\n",
      "Epoch [1/20], Step [1500/1500], Training Accuracy: 79.80\n",
      "Epoch [2/20], Step [750/1500], Training Accuracy: 84.80\n",
      "Epoch [2/20], Step [1500/1500], Training Accuracy: 86.00\n",
      "Epoch [3/20], Step [750/1500], Training Accuracy: 86.70\n",
      "Epoch [3/20], Step [1500/1500], Training Accuracy: 88.40\n",
      "Epoch [4/20], Step [750/1500], Training Accuracy: 88.90\n",
      "Epoch [4/20], Step [1500/1500], Training Accuracy: 89.40\n",
      "Epoch [5/20], Step [750/1500], Training Accuracy: 88.60\n",
      "Epoch [5/20], Step [1500/1500], Training Accuracy: 90.50\n",
      "Epoch [6/20], Step [750/1500], Training Accuracy: 90.90\n",
      "Epoch [6/20], Step [1500/1500], Training Accuracy: 88.60\n",
      "Epoch [7/20], Step [750/1500], Training Accuracy: 90.60\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:87.02%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Timescale RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinearity: Sigmoid\n",
    "\n",
    "$z_t = \\sigma (W_z r_t + P_zx_t + b_z)$ with $W_z = P_z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class multiscale_RNN_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(multiscale_RNN_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # Wz is defined in the forward function\n",
    "        self.W_z = torch.nn.Parameter(torch.zeros(self.hidden_size, self.hidden_size), requires_grad=False)\n",
    "        self.P_z = torch.nn.Parameter(torch.zeros(self.hidden_size, input_size), requires_grad=False)\n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))         \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "        self.z_low = torch.tensor(0.1)\n",
    "        self.z_high = torch.tensor(0.9)\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)*self.Sigmoid(torch.matmul(self.W_z, self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class multiscale_RNN_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(multiscale_RNN_batch, self).__init__()\n",
    "        self.rnncell = multiscale_RNN_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class multiscale_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(multiscale_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = multiscale_RNN_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = multiscale_RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiscale_RNN(\n",
      "  (lstm): multiscale_RNN_batch(\n",
      "    (rnncell): multiscale_RNN_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/20], Step [750/1500], Training Accuracy: 33.40\n",
      "Epoch [1/20], Step [1500/1500], Training Accuracy: 55.10\n",
      "Epoch [2/20], Step [750/1500], Training Accuracy: 60.70\n",
      "Epoch [2/20], Step [1500/1500], Training Accuracy: 67.40\n",
      "Epoch [3/20], Step [750/1500], Training Accuracy: 70.00\n",
      "Epoch [3/20], Step [1500/1500], Training Accuracy: 69.00\n",
      "Epoch [4/20], Step [750/1500], Training Accuracy: 74.70\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            # (batch_size, sequence_length, input_size) -> (batch_size, sequence_length*input_size/stride, input_size)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:86.28%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_1_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_1_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(1.0)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_1_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_1_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_1_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_1(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_1_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_1(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_1(\n",
      "  (lstm): vanilla_RNN_1_batch(\n",
      "    (rnncell): vanilla_RNN_1_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 57.90\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 69.50\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 74.40\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 77.20\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 78.60\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 80.10\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 80.30\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 82.00\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 81.00\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 83.80\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 83.10\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 81.00\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:82.73%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_2_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_2_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(0.5)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_2_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_2_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_2_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_2_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_2(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_2(\n",
      "  (lstm): vanilla_RNN_2_batch(\n",
      "    (rnncell): vanilla_RNN_2_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 41.50\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 61.90\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 70.20\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 74.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 76.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 79.70\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 79.30\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 78.60\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:80.62%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "class vanilla_RNN_3_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(vanilla_RNN_3_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.r_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        for W in [self.W, self.P]:\n",
    "            glorot_init(W)\n",
    "            \n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.r_t.dim() == 3:           \n",
    "            self.r_t = self.r_t[0]\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)\n",
    "        self.z_t = torch.tensor(0.1)\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.r_t = (1 - self.z_t) * self.r_t + self.z_t * self.Sigmoid(torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v)\n",
    "        self.r_t = torch.transpose(self.r_t, 0, 1)                \n",
    "\n",
    "class vanilla_RNN_3_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(vanilla_RNN_3_batch, self).__init__()\n",
    "        self.rnncell = vanilla_RNN_3_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.r_t             \n",
    "            \n",
    "class vanilla_RNN_3(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(vanilla_RNN_3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = vanilla_RNN_3_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.r_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        \n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = vanilla_RNN_3(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla_RNN_3(\n",
      "  (lstm): vanilla_RNN_3_batch(\n",
      "    (rnncell): vanilla_RNN_3_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 10.30\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 11.40\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 23.60\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 25.30\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 36.00\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 35.50\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 47.40\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 51.10\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 55.60\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 54.90\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 58.30\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 61.10\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 62.70\n",
      "Epoch [7/10], Step [1500/1500], Training Accuracy: 63.60\n",
      "Epoch [8/10], Step [750/1500], Training Accuracy: 66.30\n",
      "Epoch [8/10], Step [1500/1500], Training Accuracy: 67.20\n",
      "Epoch [9/10], Step [750/1500], Training Accuracy: 69.40\n",
      "Epoch [9/10], Step [1500/1500], Training Accuracy: 68.90\n",
      "Epoch [10/10], Step [750/1500], Training Accuracy: 70.80\n",
      "Epoch [10/10], Step [1500/1500], Training Accuracy: 70.70\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:73.45%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "\n",
    "class CB_GRU_cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_GRU_cell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # Rest gate r_t \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.rand(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K is always positive            \n",
    "        self.b_z = torch.nn.Parameter(torch.rand(self.hidden_size, 1))     \n",
    "        self.K = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.rand(self.hidden_size, input_size))\n",
    "        self.z_high = torch.tensor(0.005)\n",
    "        self.z_low = torch.tensor(1.0)\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # Nonlinear functions\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.Sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "        # No sign constraint on K and W\n",
    "\n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "        \n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = (self.z_high-self.z_low)* self.Sigmoid(torch.matmul(self.K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z) + self.z_low\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)                \n",
    "\n",
    "class CB_GRU_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_GRU_batch, self).__init__()\n",
    "        self.rnncell = CB_GRU_cell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_GRU_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "model = CB_GRU(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_GRU(\n",
      "  (lstm): CB_GRU_batch(\n",
      "    (rnncell): CB_GRU_cell(\n",
      "      (Sigmoid): Sigmoid()\n",
      "      (Tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 54.70\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 63.50\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 66.50\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 70.80\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 71.90\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 77.00\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 78.80\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 78.50\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 80.30\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 80.40\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 80.90\n",
      "Epoch [6/10], Step [1500/1500], Training Accuracy: 80.90\n",
      "Epoch [7/10], Step [750/1500], Training Accuracy: 82.80\n",
      "Epoch [7/10], Step [1500/1500], Training Accuracy: 81.20\n",
      "Epoch [8/10], Step [750/1500], Training Accuracy: 80.50\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:80.96%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n",
    "\n",
    "# Retrieve weights\n",
    "P = model.lstm.rnncell.P.detach().cpu().numpy()\n",
    "W = model.lstm.rnncell.W.detach().cpu().numpy()\n",
    "read_out = model.fc.weight.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-RNN-tied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CB_RNN_tiedcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_RNN_tiedcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.W = torch.nn.Parameter(torch.empty(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and P_z become tied          \n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # initialise e as a random float between 0 and 1\n",
    "        self.e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_p = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "        # Voltage rate\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        for w in self.W, self.P:\n",
    "            glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        e = self.softplus(self.e)\n",
    "        e_p = self.softplus(self.e_p)\n",
    "        K = e * self.softplus(self.W)\n",
    "        P_z = e_p * self.softplus(self.P)\n",
    "\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low) * self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(P_z, x) + self.b_z)\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)                \n",
    "\n",
    "class CB_RNN_tied_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_RNN_tied_batch, self).__init__()\n",
    "        self.rnncell = CB_RNN_tiedcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_RNN_tied(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_RNN_tied, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_RNN_tied_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = CB_RNN_tied(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_RNN_tied(\n",
      "  (lstm): CB_RNN_tied_batch(\n",
      "    (rnncell): CB_RNN_tiedcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/20], Step [750/1500], Training Accuracy: 58.40\n",
      "Epoch [1/20], Step [1500/1500], Training Accuracy: 69.00\n",
      "Epoch [2/20], Step [750/1500], Training Accuracy: 70.70\n",
      "Epoch [2/20], Step [1500/1500], Training Accuracy: 74.50\n",
      "Epoch [3/20], Step [750/1500], Training Accuracy: 74.50\n",
      "Epoch [3/20], Step [1500/1500], Training Accuracy: 79.20\n",
      "Epoch [4/20], Step [750/1500], Training Accuracy: 80.40\n",
      "Epoch [4/20], Step [1500/1500], Training Accuracy: 78.80\n",
      "Epoch [5/20], Step [750/1500], Training Accuracy: 46.30\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:46.7%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "labelslist = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dale-CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dale_CBcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Dale_CBcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and W are unbounded free parameters   \n",
    "        # C represents  current based portion of connectivity       \n",
    "        self.K = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "        self.C = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "        self.P_z = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # Potentials are initialised with right signs\n",
    "        self.e_e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_i = torch.nn.Parameter(-torch.rand(1))\n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "        # initialise matrices\n",
    "        # P and P_z are unconstrained\n",
    "        for w in self.P_z, self.P:\n",
    "            glorot_init(w)\n",
    "        for w in self.K, self.C:\n",
    "            positive_glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "    def init_dale(self, rows, cols):\n",
    "        # Dale's law with equal excitatory and inhibitory neurons\n",
    "        exci = torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        inhi = -torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        weights = torch.cat((exci, inhi), dim=1)\n",
    "        weights = self.adjust_spectral(weights)\n",
    "        return weights\n",
    "\n",
    "    def adjust_spectral(self, weights, desired_radius=1.5):\n",
    "        #values, _ = torch.linalg.eig(weights @ weights.T)\n",
    "        values = torch.linalg.svdvals(weights)\n",
    "        radius = values.abs().max()\n",
    "        return weights * (desired_radius / radius)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        K = self.softplus(self.K)\n",
    "        C = self.softplus(self.C)\n",
    "        # W is constructed using e*(K+C)\n",
    "        W_E = self.e_e * (K[:, :self.hidden_size//2] + C[:, :self.hidden_size//2])\n",
    "        W_I = self.e_i * (K[:, self.hidden_size//2:] + C[:, self.hidden_size//2:])\n",
    "        # print to see which device the tensor is on\n",
    "        # If sign of W do not obey Dale's law, then these terms to be 0\n",
    "        W_E = self.relu(W_E)\n",
    "        W_I = -self.relu(-W_I)\n",
    "        W = torch.cat((W_E, W_I), 1)\n",
    "        self.W = W\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//4:self.hidden_size//2,:] = 0\n",
    "        input_mask[3*self.hidden_size//4:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)* self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(W, self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "        excitatory = self.v_t[:, :self.hidden_size//2]\n",
    "        self.excitatory = torch.cat((excitatory, torch.zeros_like(excitatory)), 1)   \n",
    "\n",
    "class Dale_CB_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(Dale_CB_batch, self).__init__()\n",
    "        self.rnncell = Dale_CBcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.excitatory            \n",
    "            \n",
    "class Dale_CB(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(Dale_CB, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = Dale_CB_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        # output mask\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,:self.hidden_size//4] = 0\n",
    "        output_mask[:,3*self.hidden_size//4:] = 0        \n",
    "        out = out * output_mask\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = Dale_CB(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dale_CB(\n",
      "  (lstm): Dale_CB_batch(\n",
      "    (rnncell): Dale_CBcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 10.30\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 9.60\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:80.93%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB-RNN-tied-STP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model Definition'\n",
    "\n",
    "class CB_RNN_tiedcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CB_RNN_tiedcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.W = torch.nn.Parameter(torch.empty(self.hidden_size, self.hidden_size))\n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and P_z become tied          \n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # initialise e as a random float between 0 and 1\n",
    "        self.e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_p = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "        # Voltage rate\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        for w in self.W, self.P:\n",
    "            glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        ### STP Model ###\n",
    "        self.delta_t = 1\n",
    "        self.z_min = 0.001\n",
    "        self.z_max = 0.1\n",
    "\n",
    "        # Short term Depression parameters  \n",
    "        self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "        # Short term Facilitation parameters\n",
    "        self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        \n",
    "        # State initialisations\n",
    "        self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32).to(device)\n",
    "        self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32).to(device)\n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.Ucapclone = self.Ucap.clone().detach() \n",
    "\n",
    "        #self.X_history = []\n",
    "        #self.U_history = []\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        e = self.softplus(self.e)\n",
    "        e_p = self.softplus(self.e_p)\n",
    "        K = e * self.softplus(self.W)\n",
    "        P_z = e_p * self.softplus(self.P)\n",
    "\n",
    "        ### STP model ###\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        \n",
    "        # Short term Depression \n",
    "        self.z_x = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_x)\n",
    "        self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.r_t\n",
    "\n",
    "        # Short term Facilitation \n",
    "        self.z_u = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_u)    \n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.r_t\n",
    "        self.Ucapclone = self.Ucap.clone().detach()\n",
    "        self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "\n",
    "        ### Update Equations ###\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//2:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low)* self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(P_z, x) + self.b_z)\n",
    "        # mask p with second half of the neuron not receiving input\n",
    "\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(self.W, self.U*self.X*self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "\n",
    "        #self.X_history.append(self.X.clone().detach())\n",
    "        #self.U_history.append(self.U.clone().detach())\n",
    "        #self.v_t_history.append(self.v_t.clone().detach())\n",
    "        #self.z_t_history.append(self.z_t.clone().detach())       \n",
    "\n",
    "class CB_RNN_tied_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(CB_RNN_tied_batch, self).__init__()\n",
    "        self.rnncell = CB_RNN_tiedcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.v_t             \n",
    "            \n",
    "class CB_RNN_tied(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CB_RNN_tied, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = CB_RNN_tied_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.X = torch.ones(self.hidden_size, x.size(0), dtype=torch.float32).to(device)\n",
    "        self.lstm.rnncell.U = (self.lstm.rnncell.Ucapclone.repeat(1, x.size(0))).to(device)\n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        # mask only the second half giving output\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, hidden_size)\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,0:self.hidden_size//2] = 0\n",
    "        out = out * output_mask\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = CB_RNN_tied(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB_RNN_tied(\n",
      "  (lstm): CB_RNN_tied_batch(\n",
      "    (rnncell): CB_RNN_tiedcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 29.20\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 44.90\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 51.60\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 62.90\n",
      "Epoch [3/10], Step [750/1500], Training Accuracy: 68.40\n",
      "Epoch [3/10], Step [1500/1500], Training Accuracy: 74.80\n",
      "Epoch [4/10], Step [750/1500], Training Accuracy: 73.20\n",
      "Epoch [4/10], Step [1500/1500], Training Accuracy: 74.90\n",
      "Epoch [5/10], Step [750/1500], Training Accuracy: 77.70\n",
      "Epoch [5/10], Step [1500/1500], Training Accuracy: 77.60\n",
      "Epoch [6/10], Step [750/1500], Training Accuracy: 76.70\n",
      "No improvement in validation accuracy for 2 epochs. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:77.93%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "X_history = []\n",
    "U_history = []\n",
    "v_t_history = []\n",
    "z_t_history = []\n",
    "labelslist = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.X_history = []\n",
    "        model.lstm.rnncell.U_history = []\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "        X_history.append(model.lstm.rnncell.X_history)\n",
    "        U_history.append(model.lstm.rnncell.U_history)\n",
    "        v_t_history.append(model.lstm.rnncell.v_t_history)\n",
    "        z_t_history.append(model.lstm.rnncell.z_t_history)\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dale-CB-STP\n",
    "Accuracy of the model:55.56% (doubled neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8957, grad_fn=<MulBackward0>)\n",
      "tensor(0.9043, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Dale_CB_STPcell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Dale_CB_STPcell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        ### Parameters ###\n",
    "        # voltage gate v_t \n",
    "        self.P = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))           \n",
    "        self.b_v = torch.nn.Parameter(torch.zeros(self.hidden_size, 1))   \n",
    "\n",
    "        # Update gate z_t\n",
    "        # K and W are unbounded free parameters   \n",
    "        # C represents  current based portion of connectivity       \n",
    "        while True:\n",
    "            self.K = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "            self.C = torch.nn.Parameter(self.init_dale(self.hidden_size, self.hidden_size))\n",
    "            nse_K = self.NSE(self.K)\n",
    "            nse_C = self.NSE(self.C)\n",
    "            if nse_K > 0.87 and nse_C > 0.87:\n",
    "                break\n",
    "        self.P_z = torch.nn.Parameter(torch.empty(self.hidden_size, input_size))\n",
    "        self.b_z = torch.nn.Parameter(torch.empty(self.hidden_size, 1))   \n",
    "        # Potentials are initialised with right signs\n",
    "        self.e_e = torch.nn.Parameter(torch.rand(1))\n",
    "        self.e_i = torch.nn.Parameter(-torch.rand(1))\n",
    "\n",
    "        # Firing rate, Scaling factor and time step initialization\n",
    "        self.v_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        self.z_low = torch.tensor(0.005)\n",
    "        self.z_high = torch.tensor(1.0)\n",
    "\n",
    "        ### Nonlinear functions ###\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### Initialisation ###\n",
    "        glorot_init = lambda w: nn.init.uniform_(w, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size)))\n",
    "        positive_glorot_init = lambda w: nn.init.uniform_(w, a=0, b=(1/math.sqrt(hidden_size)))\n",
    "\n",
    "        # initialise matrices\n",
    "        # P and P_z are unconstrained\n",
    "        for w in self.P_z, self.P:\n",
    "            glorot_init(w)\n",
    "        for w in self.K, self.C:\n",
    "            positive_glorot_init(w)\n",
    "        # init b_z to be log 1/99\n",
    "        nn.init.constant_(self.b_z, torch.log(torch.tensor(1/99)))\n",
    "\n",
    "        ### STP Model ###\n",
    "        self.delta_t = 1\n",
    "        self.z_min = 0.001\n",
    "        self.z_max = 0.1\n",
    "\n",
    "        # Short term Depression parameters  \n",
    "        self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "        # Short term Facilitation parameters\n",
    "        self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "        \n",
    "        # State initialisations\n",
    "        self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32).to(device)\n",
    "        self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32).to(device)\n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.Ucapclone = self.Ucap.clone().detach() \n",
    "\n",
    "        #self.X_history = []\n",
    "        #self.U_history = []\n",
    "        #self.v_t_history = []\n",
    "        #self.z_t_history = []\n",
    "\n",
    "    def init_dale(self, rows, cols):\n",
    "        # Dale's law with equal excitatory and inhibitory neurons\n",
    "        exci = torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        inhi = -torch.empty((rows, cols//2)).exponential_(1.0)\n",
    "        weights = torch.cat((exci, inhi), dim=1)\n",
    "        weights = self.adjust_spectral(weights)\n",
    "        return weights\n",
    "\n",
    "    def adjust_spectral(self, weights, desired_radius=1.5):\n",
    "        values= torch.linalg.svdvals(weights)\n",
    "        radius = values.abs().max()\n",
    "        return weights * (desired_radius / radius)\n",
    "    \n",
    "    def NSE(self, weights):\n",
    "        values = torch.linalg.svdvals(weights)\n",
    "        normalised_v = values/sum(values)\n",
    "        H = -1/torch.log(torch.tensor(self.hidden_size)) * torch.sum(normalised_v * torch.log(normalised_v))\n",
    "        print(H)\n",
    "        return H\n",
    "\n",
    "    @property\n",
    "    def r_t(self):\n",
    "        return self.sigmoid(self.v_t)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.v_t.dim() == 3:           \n",
    "            self.v_t = self.v_t[0]\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)\n",
    "\n",
    "        ### Constraints###\n",
    "        K = self.softplus(self.K)\n",
    "        C = self.softplus(self.C)\n",
    "        # W is constructed using e*(K+C)\n",
    "        # first half of neurons are excitatory and second half are inhibitory\n",
    "        W_E = self.e_e * (K[:, :self.hidden_size//2] + C[:, :self.hidden_size//2])\n",
    "        W_I = self.e_i * (K[:, self.hidden_size//2:] + C[:, self.hidden_size//2:])\n",
    "        # print to see which device the tensor is on\n",
    "        # If sign of W do not obey Dale's law, then these terms to be 0\n",
    "        W_E = self.relu(W_E)\n",
    "        W_I = -self.relu(-W_I)\n",
    "        W = torch.cat((W_E, W_I), 1)\n",
    "        self.W = W\n",
    "\n",
    "        ### STP model ###\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        \n",
    "        # Short term Depression \n",
    "        self.z_x = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_x)\n",
    "        self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.r_t\n",
    "\n",
    "        # Short term Facilitation \n",
    "        self.z_u = self.z_min + (self.z_max - self.z_min) * self.sigmoid(self.c_u)    \n",
    "        self.Ucap = 0.9 * self.sigmoid(self.c_U)\n",
    "        self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.r_t\n",
    "        self.Ucapclone = self.Ucap.clone().detach()\n",
    "        self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "\n",
    "\n",
    "        ### Update Equations ###\n",
    "        self.z_t = torch.zeros(self.hidden_size, 1)\n",
    "        self.z_t = self.z_low + (self.z_high - self.z_low) * self.sigmoid(torch.matmul(K , self.r_t) + torch.matmul(self.P_z, x) + self.b_z)\n",
    "        \n",
    "        # input mask\n",
    "        # we want this to be orthogonal to the E/I split, so zero out half of excitatory neurons and half of inhibitory neurons\n",
    "        input_mask = torch.ones_like(self.P)\n",
    "        input_mask[self.hidden_size//4:self.hidden_size//2,:] = 0\n",
    "        input_mask[3*self.hidden_size//4:,:] = 0\n",
    "        P = self.P * input_mask\n",
    "\n",
    "        self.v_t = (1 - self.z_t) * self.v_t + torch.matmul(W, self.U*self.X*self.r_t) + torch.matmul(P, x) + self.b_v\n",
    "        self.v_t = torch.transpose(self.v_t, 0, 1)      \n",
    "        excitatory = self.v_t[:, :self.hidden_size//2]\n",
    "        self.excitatory = torch.cat((excitatory, torch.zeros_like(excitatory)), 1)    \n",
    "\n",
    "        #self.X_history.append(self.X.clone().detach())\n",
    "        #self.U_history.append(self.U.clone().detach())\n",
    "        #self.v_t_history.append(self.v_t.clone().detach())\n",
    "        #self.z_t_history.append(self.z_t.clone().detach())   \n",
    "\n",
    "\n",
    "class Dale_CB_STP_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(Dale_CB_STP_batch, self).__init__()\n",
    "        self.rnncell = Dale_CB_STPcell(input_size, hidden_size, num_layers).to(device)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_first == True:\n",
    "            for n in range(x.size(1)):\n",
    "                #print(x.shape)\n",
    "                x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "                self.rnncell(x_slice)\n",
    "        return self.rnncell.excitatory            \n",
    "            \n",
    "class Dale_CB_STP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(Dale_CB_STP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = Dale_CB_STP_batch(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        self.lstm.rnncell.X = torch.ones(self.hidden_size, x.size(0), dtype=torch.float32).to(device)\n",
    "        self.lstm.rnncell.U = (self.lstm.rnncell.Ucapclone.repeat(1, x.size(0))).to(device)\n",
    "        self.lstm.rnncell.v_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, hidden_size)\n",
    "        output_mask = torch.ones_like(out)\n",
    "        output_mask[:,:self.hidden_size//4] = 0\n",
    "        output_mask[:,3*self.hidden_size//4:] = 0        \n",
    "        out = out * output_mask\n",
    "\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "        \n",
    "        pass                                    \n",
    "pass\n",
    "\n",
    "model = Dale_CB_STP(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dale_CB_STP(\n",
      "  (lstm): Dale_CB_STP_batch(\n",
      "    (rnncell): Dale_CB_STPcell(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus): Softplus(beta=1, threshold=20)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Step [750/1500], Training Accuracy: 19.50\n",
      "Epoch [1/10], Step [1500/1500], Training Accuracy: 38.50\n",
      "Epoch [2/10], Step [750/1500], Training Accuracy: 50.60\n",
      "Epoch [2/10], Step [1500/1500], Training Accuracy: 53.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m train_acc\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_acc\n\u001b[1;32m---> 94\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, model, loaders, patience, min_delta)\u001b[0m\n\u001b[0;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yawen cheng\\Python_3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 185\u001b[0m, in \u001b[0;36mDale_CB_STP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mrnncell\u001b[38;5;241m.\u001b[39mv_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Passing in the input and hidden state into the model and  obtaining outputs\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# out: tensor of shape (batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m    186\u001b[0m output_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(out)\n\u001b[0;32m    187\u001b[0m output_mask[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yawen cheng\\Python_3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 166\u001b[0m, in \u001b[0;36mDale_CB_STP_batch.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         x_slice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(x[:,n,:], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnncell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_slice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnncell\u001b[38;5;241m.\u001b[39mexcitatory\n",
      "File \u001b[1;32mc:\\Users\\yawen cheng\\Python_3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 125\u001b[0m, in \u001b[0;36mDale_CB_STPcell.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_x \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_t \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_t\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Short term Facilitation \u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz_u\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_min \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_min) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_u)    \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUcap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_U)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUcap \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_u \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_u), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_t \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUcap \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_t\n",
      "File \u001b[1;32mc:\\Users\\yawen cheng\\Python_3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1617\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1613\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m-> 1617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1618\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_from\u001b[39m(\u001b[38;5;241m*\u001b[39mdicts_or_sets):\n\u001b[0;32m   1619\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dicts_or_sets:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'Training'\n",
    "print(model)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "'Trajactory Tracking and Training'\n",
    "from torch import optim\n",
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def subset_loader(full_dataset, batch_size, subset_ratio=0.1):\n",
    "    # Generate labels array to use in stratified split\n",
    "    labels = []\n",
    "    for _, label in full_dataset:\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Perform a stratified shuffle split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_ratio, random_state=0)\n",
    "    for train_index, test_index in sss.split(np.zeros(len(labels)), labels):\n",
    "        stratified_subset_indices = test_index\n",
    "\n",
    "    # Create a Subset instance with the stratified subset indices\n",
    "    stratified_subset = Subset(full_dataset, stratified_subset_indices)\n",
    "\n",
    "    # Create DataLoader from the subset\n",
    "    subset_loader = DataLoader(\n",
    "        stratified_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle as we already have a random subset\n",
    "    )\n",
    "\n",
    "    return subset_loader\n",
    "subtest = subset_loader(test_data, batch_size)\n",
    "\n",
    "def evaluate_while_training(model, loaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subtest:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(num_epochs, model, loaders, patience=2, min_delta=0.01):\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    train_acc = []\n",
    "    best_acc = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            images = time_gap(images, input_size, timegap).to(device)\n",
    "            images = stride(images, stride_number).to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            if (i+1) % 750 == 0:\n",
    "                accuracy = evaluate_while_training(model, loaders)\n",
    "                train_acc.append(accuracy)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.2f}' \n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, accuracy))\n",
    "\n",
    "                # Check for improvement\n",
    "                if accuracy - best_acc > min_delta:\n",
    "                    best_acc = accuracy\n",
    "                    no_improve_epochs = 0\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"No improvement in validation accuracy for {} epochs. Stopping training.\".format(patience))\n",
    "                    return train_acc\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "train_acc = train(num_epochs, model, loaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:71.63%\n"
     ]
    }
   ],
   "source": [
    "'Testing Accuracy'\n",
    "# Test the model\n",
    "model.eval()\n",
    "X_history = []\n",
    "U_history = []\n",
    "v_t_history = []\n",
    "z_t_history = []\n",
    "labelslist = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loaders['test']:\n",
    "        model.lstm.rnncell.X_history = []\n",
    "        model.lstm.rnncell.U_history = []\n",
    "        model.lstm.rnncell.v_t_history = []\n",
    "        model.lstm.rnncell.z_t_history = []\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        images = time_gap(images, input_size, timegap).to(device)\n",
    "        images = stride(images, stride_number).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "        X_history.append(model.lstm.rnncell.X_history)\n",
    "        U_history.append(model.lstm.rnncell.U_history)\n",
    "        v_t_history.append(model.lstm.rnncell.v_t_history)\n",
    "        z_t_history.append(model.lstm.rnncell.z_t_history)\n",
    "        labelslist.append(labels)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print('Accuracy of the model:{}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance of permuted MNIST')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# rotate model name\n",
    "\n",
    "model_name = ['simple GRU', 'CB-GRU', 'CB-RNN-tied', 'Dale-CB', 'CB-RNN-tied STP', 'Dale-CB STP', 'multiscale-Vanilla RNN', 'Vanilla RNN $z=1.0$', 'Vanilla RNN $z=0.5$', 'Vanilla RNN $z=0.1$']\n",
    "perf1 = [87.02, 84.53, 85.55, 10, 77.93, 60.36, 77.38, 82.73, 80.62, 73.45]\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(model_name, perf1, bar_width, label='48 Neurons')\n",
    "index = np.arange(len(model_name))\n",
    "# text bar to show input size, stride number, and hidden size\n",
    "plt.text(4.5, 80, f'Input size: {input_size}\\nStride number: {stride_number}'.format(input_size=input_size, stride_number=stride_number, hidden_size=hidden_size), ha='center', va='bottom')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance of permuted MNIST')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
