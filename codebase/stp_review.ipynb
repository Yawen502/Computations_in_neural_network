{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_STP review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### This jupyter notebook stands for the purpose to learn the code MNIST_STP and divide it into several important sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# get index of currently selected device\n",
    "print(torch.cuda.current_device()) # returns 0 in my case\n",
    "\n",
    "# get number of GPUs available\n",
    "print(torch.cuda.device_count()) # returns 1 in my case\n",
    "\n",
    "# get the name of the device\n",
    "print(torch.cuda.get_device_name(0)) # good old Tesla K80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and dataloggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x21d94086790>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x21d8c503410>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0),\n",
    "}\n",
    "loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F      \n",
    "\n",
    "sequence_length = 196\n",
    "input_size = 4\n",
    "hidden_size = 144\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Class STP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the illustrated version of code, dont run it -- too many illustrations\n",
    "class STPCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, complexity, e_h, alpha):\n",
    "        super(STPCell, self).__init__()\n",
    "        ## define the input arguments to be the artibutes in class\n",
    "        self.input_size = input_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.complexity = complexity \n",
    "        sigmoid = nn.Sigmoid() \n",
    "        self.ones = torch.ones(self.hidden_size, self.hidden_size)\n",
    "        self.batch_size = batch_size \n",
    "        self.forprintingX = []\n",
    "        self.forprintingU = []\n",
    "        self.forprintingh = []\n",
    "\n",
    "        if self.complexity == \"rich\":\n",
    "            # System variables \n",
    "            self.e_h = e_h\n",
    "\n",
    "            # Short term Plasticity variables \n",
    "            self.delta_t = 1\n",
    "            self.alpha = alpha\n",
    "            self.e_ux = self.alpha * self.e_h\n",
    "            self.z_min = 0.001\n",
    "            self.z_max = 0.1\n",
    "\n",
    "            # Short term Depression parameters  \n",
    "            ## Parameters initialised in this way are all trainable parameters\n",
    "            self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "\n",
    "            # Short term Facilitation parameters\n",
    "            self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            \n",
    "            # System parameters            \n",
    "            self.c_h = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.w = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.p = torch.nn.Parameter(torch.rand(self.hidden_size, self.input_size))  \n",
    "            self.b = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # State initialisations\n",
    "            ## h_t is a horizontal tensor! not a vertical one as we used in algebra\n",
    "            ## transpose might be needed\n",
    "            \"\"\"\"\n",
    "            h_t starts with 0, with a clean memory state\n",
    "            X starts with 1, as available resourse is 1 at first\n",
    "            U follows the same principle, as baseline utilisation factor is 0.9\n",
    "            Afterall it depends on the biological models\n",
    "            \"\"\"\n",
    "            self.h_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "            self.X = torch.ones(self.hidden_size, self.hidden_size, dtype=torch.float32)     \n",
    "            self.U = torch.full((self.hidden_size, self.hidden_size), 0.9, dtype=torch.float32)         \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            ## detach() -- don't want to include it in the gradient computation\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "        if self.complexity == \"poor\":\n",
    "            # System variables \n",
    "            self.e_h = e_h\n",
    "\n",
    "            # Short term Plasticity variables \n",
    "            self.delta_t = 1\n",
    "            self.alpha = alpha\n",
    "            self.e_ux = self.alpha * self.e_h\n",
    "            self.z_min = 0.001\n",
    "            self.z_max = 0.1\n",
    "\n",
    "            # Short term Depression parameters  \n",
    "            self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "            # Short term Facilitation parameters\n",
    "            self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # System parameters\n",
    "            self.c_h = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.w = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.p = torch.nn.Parameter(torch.rand(self.hidden_size, self.input_size))\n",
    "            self.b = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # State initialisations\n",
    "            self.h_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "            self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32)\n",
    "            self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32)   \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "        for name, param in self.named_parameters():\n",
    "            #print(name, param.size(), param)\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size))) \n",
    "\n",
    "    def forward(self, x):                 \n",
    "        if self.complexity == \"rich\":\n",
    "            if self.h_t.dim() == 3:\n",
    "                self.h_t = self.h_t[0]\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            # graph plotting \n",
    "            '''self.forprintingX.append(self.X[20,11,24].item())\n",
    "            self.forprintingU.append(self.U[20,11,24].item())\n",
    "            self.forprintingh.append(self.h_t[11, 20].item())\n",
    "            if len(self.forprintingX) % (196*5) == 0:\n",
    "                self.forprintingX = []\n",
    "                self.forprintingU = []\n",
    "                self.forprintingh = []'''   \n",
    "\n",
    "            # Short term Depression \n",
    "            self.z_x = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_x)\n",
    "            #print(\"z_x\", self.z_x.size())\n",
    "            #print(\"self.X\", self.X.size())\n",
    "            #print(\"self.ones\", self.ones.size())\n",
    "            #print(\"h_t\", self.h_t.size())\n",
    "            #print(\"self.U\", self.U.size())\n",
    "            #a = self.delta_t * self.U * torch.einsum(\"ijk, ji  -> ijk\", self.X, self.h_t)\n",
    "            #print(\"a\", a)\n",
    "            #print(\"a size\", a.size())\n",
    "            self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * torch.einsum(\"ijk, ji  -> ijk\", self.X, self.h_t)\n",
    "\n",
    "            # Short term Facilitation \n",
    "            self.z_u = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_u)    \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * torch.einsum(\"ijk, ji  -> ijk\", (1 - self.U), self.h_t)\n",
    "            self.Ucapclone = self.Ucap.clone().detach() \n",
    "            self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(self.U.size(0), 1, 1).to(device), max=torch.ones_like(self.Ucapclone.repeat(self.U.size(0), 1, 1).to(device)))\n",
    "\n",
    "            # System Equations \n",
    "            self.z_h = self.e_h * sigmoid(self.c_h) \n",
    "            #   a = self.w * self.U * self.X\n",
    "            #print(\"size of a\", a.size())\n",
    "            #print(\"size of h_t\", self.h_t.size())\n",
    "            #print(\"size of a * h_t\", torch.matmul(a, self.h_t).size())\n",
    "            #print(\"size of x\", x.size())\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            self.h_t = torch.mul((1 - self.z_h), self.h_t) + self.z_h * sigmoid(torch.einsum(\"ijk, ki  -> ji\", (self.w * self.U * self.X), self.h_t) + torch.matmul(self.p, x) + self.b)\n",
    "            #self.h_t = torch.matmul(self.w, self.h_t) + torch.matmul(self.p, x) + self.b\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            return self.h_t   \n",
    "\n",
    "        if self.complexity == \"poor\":\n",
    "            if self.h_t.dim() == 3:\n",
    "                self.h_t = self.h_t[0]\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            # Short term Depression \n",
    "            self.z_x = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_x)\n",
    "            #print(\"z_x\", self.z_x.size())\n",
    "            #print(\"self.X\", self.X.size())\n",
    "            #print(\"self.ones\", self.ones.size())\n",
    "            #print(\"self.U\", self.U.size())\n",
    "            #print(\"h_t\", self.h_t.size())\n",
    "            a = self.delta_t * self.U * self.X * self.h_t\n",
    "            #print(\"a\", a)\n",
    "            #print(\"a size\", a.size())\n",
    "        \n",
    "            self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.h_t\n",
    "\n",
    "            # Short term Facilitation \n",
    "            self.z_u = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_u)    \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.h_t\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "            self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "            # graph plotting \n",
    "            '''self.forprintingX.append(self.X[20,5].item())\n",
    "            self.forprintingU.append(self.U[20,5].item())\n",
    "            if len(self.forprintingX) % 140 == 0:\n",
    "                self.forprintingX = []\n",
    "                self.forprintingU = []'''\n",
    "\n",
    "            # System Equations \n",
    "            # self.z_h = self.e_h * sigmoid(self.c_h) \n",
    "            #a = self.w * self.U * self.X\n",
    "            #print(\"size of a\", a.size())\n",
    "            #print(\"size of h_t\", self.h_t.size())\n",
    "            #print(\"size of a * h_t\", torch.matmul(a, self.h_t).size())\n",
    "            #print(\"size of x\", x.size())\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            self.h_t = torch.mul((1 - self.c_h), self.h_t) + self.c_h * sigmoid(torch.matmul(self.w, (self.U * self.X * self.h_t)) + torch.matmul(self.p, x) + self.b)\n",
    "            #self.h_t = torch.matmul(self.w, self.h_t) + torch.matmul(self.p, x) + self.b\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            return self.h_t\n",
    "\n",
    "class STP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, complexity, e_h, alpha): \n",
    "        super(STP, self).__init__()\n",
    "        self.stpcell = STPCell(input_size, hidden_size, complexity, e_h, alpha).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for n in range(x.size(1)):\n",
    "            x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "            self.stpcell(x_slice)\n",
    "        return self.stpcell.h_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class STPCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, complexity, e_h, alpha):\n",
    "        super(STPCell, self).__init__()\n",
    "        self.input_size = input_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.complexity = complexity \n",
    "        sigmoid = nn.Sigmoid() \n",
    "        self.ones = torch.ones(self.hidden_size, self.hidden_size)\n",
    "        self.batch_size = batch_size \n",
    "        self.forprintingX = []\n",
    "        self.forprintingU = []\n",
    "        self.forprintingh = []\n",
    "\n",
    "        if self.complexity == \"rich\":\n",
    "            # System variables \n",
    "            self.e_h = e_h\n",
    "\n",
    "            # Short term Plasticity variables \n",
    "            self.delta_t = 1\n",
    "            self.alpha = alpha\n",
    "            self.e_ux = self.alpha * self.e_h\n",
    "            self.z_min = 0.001\n",
    "            self.z_max = 0.1\n",
    "\n",
    "            # Short term De,////////////////pression parameters  \n",
    "            self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "\n",
    "            # Short term Facilitation parameters\n",
    "            self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            \n",
    "            # System parameters            \n",
    "            self.c_h = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.w = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.p = torch.nn.Parameter(torch.rand(self.hidden_size, self.input_size))  \n",
    "            self.b = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # State initialisations\n",
    "            self.h_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "            self.X = torch.ones(self.hidden_size, self.hidden_size, dtype=torch.float32)     \n",
    "            self.U = torch.full((self.hidden_size, self.hidden_size), 0.9, dtype=torch.float32)         \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "        if self.complexity == \"poor\":\n",
    "            # System variables \n",
    "            self.e_h = e_h\n",
    "\n",
    "            # Short term Plasticity variables \n",
    "            self.delta_t = 1\n",
    "            self.alpha = alpha\n",
    "            self.e_ux = self.alpha * self.e_h\n",
    "            self.z_min = 0.001\n",
    "            self.z_max = 0.1\n",
    "\n",
    "            # Short term Depression parameters  \n",
    "            self.c_x = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "\n",
    "            # Short term Facilitation parameters\n",
    "            self.c_u = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.c_U = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # System parameters\n",
    "            self.c_h = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            self.w = torch.nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "            self.p = torch.nn.Parameter(torch.rand(self.hidden_size, self.input_size))\n",
    "            self.b = torch.nn.Parameter(torch.rand(self.hidden_size, 1))\n",
    "            \n",
    "            # State initialisations\n",
    "            self.h_t = torch.zeros(1, self.hidden_size, dtype=torch.float32)\n",
    "            self.X = torch.ones(self.hidden_size, 1, dtype=torch.float32)\n",
    "            self.U = torch.full((self.hidden_size, 1), 0.9, dtype=torch.float32)   \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "        for name, param in self.named_parameters():\n",
    "            #print(name, param.size(), param)\n",
    "            nn.init.uniform_(param, a=-(1/math.sqrt(hidden_size)), b=(1/math.sqrt(hidden_size))) \n",
    "\n",
    "    def forward(self, x):                 \n",
    "        if self.complexity == \"rich\":\n",
    "            if self.h_t.dim() == 3:\n",
    "                self.h_t = self.h_t[0]\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            # graph plotting \n",
    "            '''self.forprintingX.append(self.X[20,11,24].item())\n",
    "            self.forprintingU.append(self.U[20,11,24].item())\n",
    "            self.forprintingh.append(self.h_t[11, 20].item())\n",
    "            if len(self.forprintingX) % (196*5) == 0:\n",
    "                self.forprintingX = []\n",
    "                self.forprintingU = []\n",
    "                self.forprintingh = []'''   \n",
    "\n",
    "            # Short term Depression \n",
    "            self.z_x = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_x)\n",
    "            #print(\"z_x\", self.z_x.size())\n",
    "            #print(\"self.X\", self.X.size())\n",
    "            #print(\"self.ones\", self.ones.size())\n",
    "            #print(\"h_t\", self.h_t.size())\n",
    "            #print(\"self.U\", self.U.size())\n",
    "            #a = self.delta_t * self.U * torch.einsum(\"ijk, ji  -> ijk\", self.X, self.h_t)\n",
    "            #print(\"a\", a)\n",
    "            #print(\"a size\", a.size())\n",
    "            self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * torch.einsum(\"ijk, ji  -> ijk\", self.X, self.h_t)\n",
    "\n",
    "            # Short term Facilitation \n",
    "            self.z_u = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_u)    \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * torch.einsum(\"ijk, ji  -> ijk\", (1 - self.U), self.h_t)\n",
    "            self.Ucapclone = self.Ucap.clone().detach() \n",
    "            self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(self.U.size(0), 1, 1).to(device), max=torch.ones_like(self.Ucapclone.repeat(self.U.size(0), 1, 1).to(device)))\n",
    "\n",
    "            # System Equations \n",
    "            self.z_h = self.e_h * sigmoid(self.c_h) \n",
    "            #   a = self.w * self.U * self.X\n",
    "            #print(\"size of a\", a.size())\n",
    "            #print(\"size of h_t\", self.h_t.size())\n",
    "            #print(\"size of a * h_t\", torch.matmul(a, self.h_t).size())\n",
    "            #print(\"size of x\", x.size())\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            self.h_t = torch.mul((1 - self.z_h), self.h_t) + self.z_h * sigmoid(torch.einsum(\"ijk, ki  -> ji\", (self.w * self.U * self.X), self.h_t) + torch.matmul(self.p, x) + self.b)\n",
    "            #self.h_t = torch.matmul(self.w, self.h_t) + torch.matmul(self.p, x) + self.b\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            return self.h_t   \n",
    "\n",
    "        if self.complexity == \"poor\":\n",
    "            if self.h_t.dim() == 3:\n",
    "                self.h_t = self.h_t[0]\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            # Short term Depression \n",
    "            self.z_x = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_x)\n",
    "            #print(\"z_x\", self.z_x.size())\n",
    "            #print(\"self.X\", self.X.size())\n",
    "            #print(\"self.ones\", self.ones.size())\n",
    "            #print(\"self.U\", self.U.size())\n",
    "            #print(\"h_t\", self.h_t.size())\n",
    "            a = self.delta_t * self.U * self.X * self.h_t\n",
    "            #print(\"a\", a)\n",
    "            #print(\"a size\", a.size())\n",
    "        \n",
    "            self.X = self.z_x + torch.mul((1 - self.z_x), self.X) - self.delta_t * self.U * self.X * self.h_t\n",
    "\n",
    "            # Short term Facilitation \n",
    "            self.z_u = self.z_min + (self.z_max - self.z_min) * sigmoid(self.c_u)    \n",
    "            self.Ucap = 0.9 * sigmoid(self.c_U)\n",
    "            self.U = self.Ucap * self.z_u + torch.mul((1 - self.z_u), self.U) + self.delta_t * self.Ucap * (1 - self.U) * self.h_t\n",
    "            self.Ucapclone = self.Ucap.clone().detach()\n",
    "            self.U = torch.clamp(self.U, min=self.Ucapclone.repeat(1, x.size(0)).to(device), max=torch.ones_like(self.Ucapclone.repeat(1, x.size(0)).to(device)))\n",
    "            # graph plotting \n",
    "            '''self.forprintingX.append(self.X[20,5].item())\n",
    "            self.forprintingU.append(self.U[20,5].item())\n",
    "            if len(self.forprintingX) % 140 == 0:\n",
    "                self.forprintingX = []\n",
    "                self.forprintingU = []'''\n",
    "\n",
    "            # System Equations \n",
    "            # self.z_h = self.e_h * sigmoid(self.c_h) \n",
    "            #a = self.w * self.U * self.X\n",
    "            #print(\"size of a\", a.size())\n",
    "            #print(\"size of h_t\", self.h_t.size())\n",
    "            #print(\"size of a * h_t\", torch.matmul(a, self.h_t).size())\n",
    "            #print(\"size of x\", x.size())\n",
    "            x = torch.transpose(x, 0, 1)\n",
    "            self.h_t = torch.mul((1 - self.c_h), self.h_t) + self.c_h * sigmoid(torch.matmul(self.w, (self.U * self.X * self.h_t)) + torch.matmul(self.p, x) + self.b)\n",
    "            #self.h_t = torch.matmul(self.w, self.h_t) + torch.matmul(self.p, x) + self.b\n",
    "            self.h_t = torch.transpose(self.h_t, 0, 1)\n",
    "            return self.h_t\n",
    "\n",
    "class STP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, complexity, e_h, alpha): \n",
    "        super(STP, self).__init__()\n",
    "        self.stpcell = STPCell(input_size, hidden_size, complexity, e_h, alpha).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for n in range(x.size(1)):\n",
    "            x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "            self.stpcell(x_slice)\n",
    "        return self.stpcell.h_t                                   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Hierachy: STPclass, STP and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class STP carries most of the charateristics of STP class, the only difference being the change in forward() function.\n",
    "However, RNN, in contrast, wrap the STP cell in classical RNN structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Hierachy: STPclass, STP and RNN\n",
    "class STP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, complexity, e_h, alpha): \n",
    "        super(STP, self).__init__()\n",
    "        self.stpcell = STPCell(input_size, hidden_size, complexity, e_h, alpha).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for n in range(x.size(1)):\n",
    "            x_slice = torch.transpose(x[:,n,:], 0, 1)\n",
    "            self.stpcell(x_slice)\n",
    "        return self.stpcell.h_t                                   \n",
    "            \n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = STP(input_size, hidden_size, \"rich\", 0.9, 0.1).to(device)\n",
    "        ## The purpose of this fc layer is to convert the RNN output into class\n",
    "        #  probabilities that can be used for the classification loss function during training.\n",
    "        self.fc = nn.Linear(hidden_size, num_classes).to(device)\n",
    "        self.update_number = 0\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        if self.lstm.stpcell.complexity == \"rich\":\n",
    "            ## x.size(0) is the batch_size\n",
    "            ## remember h_t is the hidden state, X is the STD available resource\n",
    "            ## and U is the baseline available resource.\n",
    "            self.lstm.stpcell.h_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            self.lstm.stpcell.X = torch.ones(x.size(0), self.hidden_size, self.hidden_size, dtype=torch.float32).to(device)\n",
    "            #self.lstm.stpcell.U = torch.full((x.size(0), self.hidden_size, self.hidden_size), 0.9, dtype=torch.float32).to(device)\n",
    "            self.lstm.stpcell.U = (self.lstm.stpcell.Ucapclone.repeat(x.size(0), 1, 1)).to(device)\n",
    "        if self.lstm.stpcell.complexity == \"poor\":\n",
    "            self.lstm.stpcell.h_t = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "            self.lstm.stpcell.X = torch.ones(self.hidden_size, x.size(0), dtype=torch.float32).to(device)\n",
    "            #self.lstm.stpcell.U = torch.full((self.hidden_size, x.size(0)), 0.9, dtype=torch.float32).to(device)\n",
    "            self.lstm.stpcell.U = (self.lstm.stpcell.Ucapclone.repeat(1, x.size(0))).to(device)\n",
    "            #torch.full((2, 3), 3.141592)\n",
    "        '''self.update_number += 1 \n",
    "        if self.update_number % 50 == 0: \n",
    "            plt.plot(self.lstm.stpcell.forprintingX)\n",
    "            plt.plot(self.lstm.stpcell.forprintingU)\n",
    "            plt.plot(self.lstm.stpcell.forprintingh)\n",
    "            plt.legend([\"X\",\"U\",\"h_t\"])\n",
    "            plt.show()'''\n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
    "        out = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "        pass                                    \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseindexing\n",
    "This function converts the 2D image to a sequential input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseindexing(time_gap, input_size, stride, a):\n",
    "    a = a.flatten()\n",
    "    ## put from gpu to cpu because np only works in cpu\n",
    "    a = a.cpu()\n",
    "    ## convert cpu tensors to numpy array\n",
    "    a = a.numpy()\n",
    "\n",
    "    baseinds = np.arange(0, time_gap*input_size, time_gap)\n",
    "\n",
    "    #zero padding \n",
    "    a = np.pad(a, (baseinds[-1],0), 'constant')\n",
    "\n",
    "    new_sequence = []\n",
    "\n",
    "    for t in range(784):\n",
    "        new_sequence.append(a[(t+baseinds).tolist()])        \n",
    "    '''print(\"baseinds\", baseinds)\n",
    "    print(\"2baseinds\", 2+baseinds)\n",
    "    print((2+baseinds).tolist())\n",
    "    print(a[(2+baseinds).tolist()])'''\n",
    "    \n",
    "    #new_sequence = [item for sublist in new_sequence for item in sublist] \n",
    "    new_sequence = np.array(new_sequence)\n",
    "    new_sequence = torch.tensor(new_sequence, dtype=torch.float).to(device)\n",
    "    #print(\"size of new sequence tensor\", new_sequence.size())\n",
    "    return new_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "1. Load data from loaders( the train dataset)\n",
    "2. reshape to fit into the model\n",
    "3. feed into the model and get outputs\n",
    "4. compare the outputs with acutal label to get loss function\n",
    "5. use gradient optimizer to do backward propagation, and step forward the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, loaders): \n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass    \n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))  \n",
    "                pass\n",
    "        \n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Simply evaluate the model by compare real and predicted model, just as what we did for loss function, but...\n",
    "1. Using *test data* instead\n",
    "2. Translated into percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model\n",
    "def evaluate(mymodel):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = mymodel(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = total + labels.size(0)\n",
    "            correct = correct + (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))                  \n",
    "    return (100*correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiralisers and spiralised train and test function\n",
    "From Claude 2:\n",
    "\n",
    "The spiraliser function takes a 2D tensor 'a' and returns a new tensor containing the values of 'a' in spiral order.\n",
    "\n",
    "Here is what it is doing step-by-step:\n",
    "\n",
    "Convert 'a' to cpu and numpy array for easier manipulation.\n",
    "Initialize loop variables k, l for starting row/col indices.\n",
    "Initialize empty list 'spiral' to store values.\n",
    "Loop through the tensor in spiral order:\n",
    "Print first row\n",
    "Print last column\n",
    "Print last row\n",
    "Print first column\n",
    "This is done by looping through the appropriate indices and appending to spiral list.\n",
    "\n",
    "Convert spiral list back to tensor with shape (28,28).\n",
    "So in summary, it takes a 28x28 2D tensor, iterates through it in spiral order appending values to a list, then converts back to 28x28 tensor containing values in spiral order.\n",
    "\n",
    "This is used to transform the MNIST images to introduce a temporal pattern that the RNN can learn. The idea is that scanning the image in spiral order creates a sequence rather than static image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiraliser(m, n, a):\n",
    "    a = a.cpu()\n",
    "    a = a.numpy()\n",
    "    k = 0\n",
    "    l = 0\n",
    "    spiral = []\n",
    "    ''' k - starting row index\n",
    "        m - ending row index\n",
    "        l - starting column index\n",
    "        n - ending column index\n",
    "        i - iterator '''\n",
    "  \n",
    "    while (k < m and l < n):\n",
    "  \n",
    "        # Print the first row from\n",
    "        # the remaining rows\n",
    "        for i in range(l, n):\n",
    "            spiral.append(a[k][i])\n",
    "  \n",
    "        k += 1\n",
    "  \n",
    "        # Print the last column from\n",
    "        # the remaining columns\n",
    "        for i in range(k, m):\n",
    "            spiral.append(a[i][n - 1])\n",
    "  \n",
    "        n -= 1\n",
    "  \n",
    "        # Print the last row from\n",
    "        # the remaining rows\n",
    "        if (k < m):\n",
    "  \n",
    "            for i in range(n - 1, (l - 1), -1):\n",
    "                spiral.append(a[m - 1][i])\n",
    "  \n",
    "            m -= 1\n",
    "  \n",
    "        # Print the first column from\n",
    "        # the remaining columns\n",
    "        if (l < n):\n",
    "            for i in range(m - 1, k - 1, -1):\n",
    "                spiral.append(a[i][l])\n",
    "  \n",
    "            l += 1\n",
    "        \n",
    "    spiraltensor = torch.tensor(spiral, dtype=torch.float)\n",
    "    spiraltensor = spiraltensor.reshape(28, 28).to(device)\n",
    "    return spiraltensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainspiral(num_epochs, model, loaders): \n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            #p = torch.rand(batch_size, 1, 784, input_size)\n",
    "            p = images.clone()\n",
    "            for n in range(images.size(dim=0)):         # Use this loop to spiralise the image \n",
    "                for image in images[n,0:1,:,:]: \n",
    "                    spiralimage = spiraliser(28, 28, image)        \n",
    "                    #indexedimage = baseindexing(3, input_size, 1, spiralimage)  \n",
    "                    p[n,0,:,:] = spiralimage\n",
    "                    #print(images[2,0:1,0:28,0:28])\n",
    "                    #print(p.size())\n",
    "                    #print(image.size())\n",
    "            images = p.clone() \n",
    "            #images = images.reshape(-1, 784, input_size).to(device)\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass    \n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))  \n",
    "                pass\n",
    "        \n",
    "        pass\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
